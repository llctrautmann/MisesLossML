{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m Dataset, DataLoader\n\u001b[0;32m----> 4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorchaudio\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mrandom\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import torchaudio\n",
    "import random\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from DGLim.hyperparameter import hp\n",
    "\n",
    "class AvianNatureSounds(Dataset):\n",
    "    def __init__(self,\n",
    "                # file args\n",
    "                annotation_file_path=None,\n",
    "                root_dir='../',\n",
    "                key='habitat',\n",
    "\n",
    "                # sound transformation args\n",
    "                mode='stft',\n",
    "                length=5,\n",
    "                sampling_rate=44100,\n",
    "                n_fft=1024,\n",
    "                hop_length=512,\n",
    "                downsample=True,\n",
    "                mel_spectrogram = None,\n",
    "                verbose=False,\n",
    "                fixed_limit=False\n",
    "                ):\n",
    "        \n",
    "        self.column = key\n",
    "        self.annotation_file = pd.read_csv(annotation_file_path).sort_values(self.column)\n",
    "        self.root_dir = root_dir\n",
    "        self.mel_transformation = mel_spectrogram\n",
    "        self.AmplitudeToDB = torchaudio.transforms.AmplitudeToDB()\n",
    "        self.mode = mode\n",
    "        self.length = length\n",
    "        self.n_fft = n_fft\n",
    "        self.hop_length = hop_length\n",
    "        self.downsample = downsample\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.verbose = verbose\n",
    "        self.fixed_limit = fixed_limit\n",
    "        self.signal_length = None\n",
    "        self.griffin_lim = torchaudio.transforms.GriffinLim(n_fft=self.n_fft, \n",
    "                                                            win_length=self.n_fft,\n",
    "                                                            hop_length=self.hop_length,\n",
    "                                                            power=2,\n",
    "                                                            n_iter=5,\n",
    "                                                            momentum=0.99)\n",
    "        self.return_signal_dims()\n",
    "\n",
    "    def return_signal_dims(self):\n",
    "        if self.verbose:\n",
    "            print(f'Returning signal dimensions: H = {self.n_fft // 2 + 1} W = {((self.sampling_rate * self.length)//self.hop_length) + 1}')\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotation_file)\n",
    "    \n",
    "    def _update_signal_length(self,signal_length):\n",
    "        self.signal_length = signal_length\n",
    "        \n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        ####################################################################################\n",
    "        # Get stft spectrograms ############################################################\n",
    "        ####################################################################################\n",
    "        if self.mode == 'stft':\n",
    "            audio_sample_path = os.path.join(self.root_dir,os.listdir(self.root_dir)[index])\n",
    "            label = self.annotation_file.iloc[index][self.column]\n",
    "            signal, sr = torchaudio.load(audio_sample_path)\n",
    "\n",
    "            if self.downsample:\n",
    "                signal = self.downsample_waveform(signal)\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "            # Clip the signal to the desired length\n",
    "            signal = self.clip(signal, sr, self.length,fixed_limit=self.fixed_limit)\n",
    "            # print(f'{signal.shape} = clipped signal shape')\n",
    "\n",
    "            stft = torch.stft(signal, n_fft=self.n_fft, hop_length=self.hop_length,win_length=self.n_fft, normalized=False, return_complex=True)\n",
    "\n",
    "\n",
    "            # Add complex Gaussian noise to the complex tensor\n",
    "            noise_real = torch.randn_like(stft.real)\n",
    "            noise_imag = torch.randn_like(stft.imag)\n",
    "            noisy_sig = stft + (noise_real + 1j * noise_imag)\n",
    "\n",
    "\n",
    "            magnitude = torch.abs(stft) # 25 Jul 2023 @ 12:21:38 ### CHANGED ###\n",
    "\n",
    "            return stft, noisy_sig,  magnitude , label\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    @torch.no_grad()\n",
    "    def clip(audio_signal, sr, desired_length,fixed_limit=False):\n",
    "        sig_len = audio_signal.shape[1]\n",
    "        length = int(sr * desired_length)\n",
    "\n",
    "        if fixed_limit:\n",
    "            sig = audio_signal[0][:262100]\n",
    "            return  sig.unsqueeze(0)\n",
    "\n",
    "        elif sig_len > length:\n",
    "            offset = random.randint(0, sig_len - length)\n",
    "            sig = audio_signal[:, offset:(offset+length)]\n",
    "\n",
    "            return sig\n",
    "        elif fixed_limit is None:\n",
    "            return audio_signal\n",
    "        \n",
    "    @staticmethod\n",
    "    @torch.no_grad()\n",
    "    def _resize_signal_length(signal, signal_length):\n",
    "        if signal.shape[-1] > signal_length:\n",
    "            signal = signal[...,:signal_length]\n",
    "            return signal\n",
    "        elif signal.shape[-1] < signal_length:\n",
    "            length_diff = signal_length - len(signal[-1])\n",
    "\n",
    "            prefix = torch.zeros((1,length_diff//2))\n",
    "            suffix = torch.zeros((1,length_diff//2))\n",
    "            signal = torch.cat([prefix,signal,suffix],dim=-1)\n",
    "\n",
    "            if len(signal[-1]) == signal_length:\n",
    "                return signal\n",
    "            else:\n",
    "                length_diff = signal_length - len(signal[-1])\n",
    "                signal = torch.cat([signal,torch.zeros((1,length_diff))],dim=-1)\n",
    "                return signal\n",
    "        else:\n",
    "            return signal\n",
    "        \n",
    "    \n",
    "    @staticmethod\n",
    "    @torch.no_grad()\n",
    "    def downsample_waveform(waveform, orig_freq=44100, new_freq=16000):\n",
    "        \"\"\"\n",
    "        Downsamples a PyTorch tensor representing a waveform.\n",
    "\n",
    "        Args:\n",
    "        waveform (Tensor): Tensor of shape (..., time) representing the waveform to be resampled.\n",
    "        orig_freq (int, optional): Original frequency of the waveform. Defaults to 44100.\n",
    "        new_freq (int, optional): Frequency to downsample to. Defaults to 16000.\n",
    "\n",
    "        Returns:\n",
    "        Tensor: Downsampled waveform.\n",
    "        \"\"\"\n",
    "        transform = torchaudio.transforms.Resample(orig_freq=orig_freq, new_freq=new_freq)\n",
    "        return transform(waveform)\n",
    "\n",
    "\n",
    "ds = AvianNatureSounds(annotation_file_path=hp.annotation_file_path,\n",
    "                       root_dir=hp.root_dir,\n",
    "                       key=hp.key,\n",
    "                       mode=hp.mode,\n",
    "                       length=hp.length,\n",
    "                       sampling_rate=hp.sampling_rate,\n",
    "                       n_fft=hp.n_fft,\n",
    "                       hop_length=hp.hop_length,\n",
    "                       mel_spectrogram=hp.mel_spectrogram,\n",
    "                       verbose=hp.verbose,\n",
    "                       fixed_limit=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
